x <- (1,3,12,42,58,61,71,89,91,98)
x <- c(1,3,12,42,58,61,71,89,91,98)
mean(x)
median(x)
x <- c(4,
28,
31,
63,
66,
68,
72,
72,
74,
77)
x
mean(x)
median(x)
x <- c(19,
27
32
51
56
59
60
66
68
83)
,
x <- c(19,
27,
32,
51,
56,
59,
60,
66,
68,
83)
mean(x)
x <-c(2,
4,
4,
7,
30,
37,
45,
64,
77,
93)
mean(x)
max(x)
median(x)
x <-c(15,
19,
29,
58,
71,
72,
88,
91,
96,
98)
median(x)
x <-c(4,
15,
17,
17,
22,
49,
54,
57,
77,
84)
median(x)
mean(x)
max(x)
library(ggplot2)
library(factoextra)
#using iris data: flowers species
data("iris")
# hierarchical clustering
dissimilarity <- dist(irist[,3:4])
# hierarchical clustering
dissimilarity <- dist(irist[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3:4)])
clusters <- hclust(dissimilarity)
clusters
dissimilarity
# plotting the dendogram
plot(clusters)
rect.hclust(clusters,k=3)
# cutting the tree to find clusters
plot(clusters)
rect.hclust(clusters,k=3)
rect.hclust(clusters,k=3,border = 1:3)
rect.hclust(clusters,k=3,border = 1:3)
###cutting the dendrogram
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
###cutting the dendrogram with specified number of clusters
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
plot(cluster_cut)
table(ccluster_cut)
table(cluster_cut)
table(cluster_cut,iris$Species)
iris_clusters2 <- hculst(dist(iris[,3:4]),method = 'average')
iris_clusters2 <- hclust(dist(iris[,3:4]),method = 'average')
plot(iris_clusters2)
rect.hclust(iris_clusters2,3)
cluster_cut2 <- cutree(iris_clusters2,3)
table(cluster_cut2,iris$Species)
table(cluster_cut,iris$Species)
table(cluster_cut2,iris$Species)
ggplot(iris,aes(x= Petal.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Peta.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
colnames(iris)
ggplot(iris,aes(x= Petal.Length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Petal.Length,y = Petal.Width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
#------------------------------------------------
### Auto data
#------------------------------------------------
# using Auto data
library(ISLR)
data("Auto")
auto
# scaling
####make variables similar scales
str(Auto)
Auto_scale <- scale(Auto[,1:7])
head(Auto)
head(Auto_scale)
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
#------------------------------------------------
### number of clusters?
#------------------------------------------------
# elbow method
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
# Silhouette method
fviz_nbclust(Auto_scaled,
kmeans,
method="silhouette")
# Gap Statistic method
fviz_nbclust(Auto_scaled,
kmeans,
method="gap_stat",
nboot=100)
library("NbClust")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladian")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucledean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
Nb_cl <- NbClust(Auto_scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
# try different clusters and compare the results
km2 <- kmeans(Auto_scale,2)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
remove(Auto_scale)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
km2 <- kmeans(Auto_scaled,2)
km3 <- kmeans(Auto_scaled,3)
km4 <- kmeans(Auto_scaled,4)
km5 <- kmeans(Auto_scaled,5)
km6 <- kmeans(Auto_scaled,6)
km7 <- kmeans(Auto_scaled,7)
# visualize the results
library(cowplot)
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p1
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p2 <- fviz_cluster(km3,data = Auto_scaled) + theme_minimal() +
ggtitle("k=3")
p3 <- fviz_cluster(km4,data = Auto_scaled) + theme_minimal() +
ggtitle("k=4")
p4 <- fviz_cluster(km5,data = Auto_scaled) + theme_minimal() +
ggtitle("k=5")
p5 <- fviz_cluster(km6,data = Auto_scaled) + theme_minimal() +
ggtitle("k=6")
p6 <- fviz_cluster(km7,data = Auto_scaled) + theme_minimal() +
ggtitle("k=7")
plot_grid(p1,p2,p3,p4,p5,p6,
labels = C("k2","k3","k4","k5","k6","k7"))
library(cowplot)
plot_grid(p1,p2,p3,p4,p5,p6)
# pick the best number of clusters
###nstart uses many iterations through kmeans to remove randomization of initial assignments
finalkm <- kmeans(Auto_scaled,3,nstart = 30)
fviz_cluster(finalkm,data = Auto_scaled)
fviz_cluster(finalkm)
fviz_cluster(finalkm,data = Auto_scaled)
# store the cluster information in the data frame
Auto$cluster <- as.factor(finalkm$cluster)
# use ggpairs to see if the clusters are meaningful
library(GGally)
ggpairs(Auto,1:5,mapping = aes(color = cluster,alpha = .5))
#------------------------------------------------
### PCA
#------------------------------------------------
# use the auto data set
data(Auto)
# run pca model
auto_pca <- prcomp(Auto)
# run pca model
auto_pca <- prcomp(Auto[,1:7])
# summary
summary(auto_pca)
# run pca model
auto_pca <- prcomp(Auto[,1:7],
center = TRUE,
scale = TRUE)
# summary
summary(auto_pca)
# how much variation each dimension captures
fviz_screeplot(auto_pca)
# Extract the results for variables
var <- get_pca_var(auto_pca)
# Contributions of variables to PC1
fviz_contrib(auto_pca,choice = "var",axes = 1,top = 10)
install.packages("devtools")
install.packages("devtools")
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
#install.packages("devtools")
#library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
# ggbiplot to visualize the results
ggbiplot(auto_pca,ellipse = TRUE)
install.packages("xgboost")  # for boosting
install.packages("FeatureHashing")  # for feature generation
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
install.packages("RSentiment")
# Load
library(ggplot2)
library(plotROC)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(xgboost)
library(FeatureHashing)
library(Matrix)
library(RSentiment)
# imdb review file is store as a tsv file.
# tsv file is very similar to csv except it uses tabs (\t)
# instead of comma as delimeter
imdb <- read.delim("labeledTrainData.tsv",quote="", as.is=T)
# imdb review file is store as a tsv file.
# tsv file is very similar to csv except it uses tabs (\t)
# instead of comma as delimeter
imdb <- read.delim("labeledTrainData.csv",quote="", as.is=T)
list.files(".")
#######################################
#                                     #
#          Sentiment Analysis         #
#                                     #
#              11/21/19               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
setwd("C:\\Users\\noahe\\Desktop\\310_mine")
# imdb review file is store as a tsv file.
# tsv file is very similar to csv except it uses tabs (\t)
# instead of comma as delimeter
imdb <- read.delim("labeledTrainData.csv",quote="", as.is=T)
# imdb review file is store as a tsv file.
# tsv file is very similar to csv except it uses tabs (\t)
# instead of comma as delimeter
imdb <- read.delim("labeledTrainData.tsv",quote="", as.is=T)
# explore the data
View(imdb)
# sample review
imdb$review[457]
strwrap(imdb$review[457], width = 80)
# ratio of positives and negatives
table(imdb$sentiment)
# a function to
#     1. remove the punctuation,
#     2. lowercase, and
#     3. create a (hashed) features.
cleantext <- function(df){
df$review <- tolower(gsub("[^[:alnum:] ]"," ", df$review))
features <- hashed.model.matrix(~ split(review, delim = " ", type = "tf-idf"),
data = imdb, hash.size = 2^16, signed.hash = FALSE)
}
dim(data)   # it resulted into more than 65000 features!
data = cleantext(imdb)
dim(data)   # it resulted into more than 65000 features!
# concatenating all the first 1000 reviews into a single string
text <- paste(imdb$review[1:1000], collapse=" ")
docs <- Corpus(VectorSource(imdb$review[1:500]))
# more cleaning
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Remove some punctuations
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "><br")
docs <- tm_map(docs, toSpace, ".<br")
docs <- tm_map(docs, toSpace, "<br")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c("movie", "film"))
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# creating the term-document matrix:
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# top 10 words in the corpus
head(d, 10)
set.seed(2019)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 100)
# find the words that are mostly correlated with "thriller"
findAssocs(dtm, terms = "thriller", corlimit = 0.3)
# create a bar plot for top 10 frequent words
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
#------------------------------------------------
### Classification Model
#------------------------------------------------
# split the data set into training and test
# choose 20000 for train and 5000 for test
set.seed(2019)
train <- sample(1:nrow(imdb), size=20000)
dtrain <- xgb.DMatrix(data[train,], label = imdb$sentiment[train])
dvalid <- xgb.DMatrix(data[-train,], label = imdb$sentiment[-train])
boosted_linear <- xgboost(booster = "gblinear", data =dtrain,
eta = 0.02, eval_metric = "error",
nrounds = 10, objective = "binary:logistic",
verbose = 1)
# boosted tree
boosted_tree <- xgboost(booster = "gbtree", data =dtrain,
max_depth = 10, eta = 0.05, nthread = 4,
colsample_bytree = 0.5, eval_metric = "error",
nrounds = 400, objective = "binary:logistic",
verbose = 1)
preds_DF <- data.frame(scores_lm = predict(boosted_linear, newdata=dvalid),
scores_tree = predict(boosted_tree, newdata=dvalid),
test_labels = imdb$sentiment[-train])
## Plot ROC curve
# boosted linear
ggplot(preds_DF,
aes(m = scores_lm,
d = test_labels)) +
geom_roc(cutoffs.at = c(.99,.9,.8,.7,.6,.5,.4,.3,.2,.1,0))
# boosted tree
ggplot(preds_DF,
aes(m = scores_tree,
d = test_labels)) +
geom_roc(cutoffs.at = c(.99,.9,.8,.7,.6,.5,.4,.3,.2,.1,0))
# accuracy
predicted_sentiment <- ifelse(preds_DF$scores_tree>0.5,"Pos","Neg")
table(predicted_sentiment,preds_DF$test_labels)
bad_review <- "This movie fucking sucks"
good_review <- "Solid movie, Definitely would watch it again"
test_df <- data.frame(review = c(bad_review,good_review))
# process the test
test_processed <- tolower(gsub("[^[:alnum:] ]"," ", test_df$review))
test_processed <- hashed.model.matrix(~ split(review, delim = " ", type = "tf-idf"),
data = test_df, hash.size = 2^16, signed.hash = FALSE)
# predict
predict(boosted_linear, newdata=test_processed)
predict(boosted_tree, newdata=test_processed)
sentences <- c("This is a good text",
"This is a bad text",
"This is a really bad text",
"This is horrible")
calculate_total_presence_sentiment(sentences)
calculate_sentiment(sentences)
calculate_score(sentences)
# Let's see if the model can detect sarcasm!
calculate_total_presence_sentiment("Really, Sherlock? No! You are clever.")
calculate_total_presence_sentiment("Nice perfume. How long did you marinate in it?")
